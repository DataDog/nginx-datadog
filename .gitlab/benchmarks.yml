variables:
  BASE_CI_IMAGE: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/benchmarking-platform:cpp-nginx

.benchmarks:
  stage: benchmarks
  tags: ["runner:apm-k8s-same-cpu"]
  timeout: 1h
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - when: manual
  image: $BASE_CI_IMAGE
  script:
    - export ARTIFACTS_DIR="$(pwd)/reports" && (mkdir "${ARTIFACTS_DIR}" || :)
    - export DD_API_KEY=$(aws ssm get-parameter --region us-east-1 --name ci.nginx-datadog.dd_api_key --with-decryption --query "Parameter.Value" --out text)
    - git clone --branch cpp/nginx https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform /platform && cd /platform
    - ./steps/capture-hardware-software-info.sh
    - ./steps/run-benchmarks.sh
    - "./steps/upload-results-to-s3.sh || :"
  artifacts:
    name: "reports"
    paths:
      - reports/
    expire_in: 3 months
  variables:
    FF_USE_LEGACY_KUBERNETES_EXECUTION_STRATEGY: "true"
    KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: nginx-datadog
    DD_BENCHMARKS_NGINX_IMAGE_TAG: nginx_1.18.0
    K6_RUN_ID_PREFIX: ci
  # Workaround: Currently we're not running the benchmarks on every PR, but GitHub still shows them as pending.
  # By marking the benchmarks as allow_failure, the Github checks are not displayed.
  allow_failure: true

baseline:
  extends: .benchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: baseline

only-tracing:
  extends: .benchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-tracing
